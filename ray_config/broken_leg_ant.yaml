agent:
  # - PPOI2LImit
  # - PPOAIRLImit
  - PPOGAIL-SASImit
  # - PPOGAILImit
  # - PPOGAIL-SImit
  - PPODARC
  # - PPOWassImit
  # - PPOWass-SImit
  # - PPOWass-SASImit
  # - PPOANE
device: cpu
seed: 42

# Ray
repeat_run: 4        # 5
metric: mean_avg_return
mode: max
# log_dir: null         # no need thanks to ray

# Sim2Real
type_training: use_one_trajectory
initialize_real_policy: True
real_sample_interval: 2000
path_optimal_sim_policy: 'envs/ant/models/PPO/actor_critic'
path_optimal_ob_rms: 'envs/ant/models/PPO/ob_rms'
experts_dir: 'expert_trajectories'
sim_max_traj_length: 1000
real_max_traj_length: 1000
sim_num_steps: 1000
real_num_steps: 1000
n_real_trajs: 1
n_sim_trajs: 8
real_num_processes: 1
sim_num_processes: 8      # because we can only sample from one real environment
n_epochs: 5000
ppo_epoch: 5
num_mini_batch: 20
mini_batch_size: null    # 256
real_mini_batch_size: null    # 256
sim_mini_batch_size: null     # 256

# DARC parameters
classifier_hidden: 100
dis_dropout: False
darc_batch_size: 1000
darc_mini_batch_size: 128
darc_grad_steps: 5
noise_std_discriminator:
  - 0.0
  - 0.1
  - 0.5
  - 1.0
darc_lr: 1.0e-3

# Imitation regularization
imit_coef:
  - 0.5
  - 1
  - 2
  - 5
type_wass_reward:
  - classic
  # - pwil

# ANE
ane_noise_std:
  - 0.1
  - 0.2
  - 0.3
  - 0.5

# GAIL
subsample_frequency: 1
gail_epoch: 5
gail_hidden: 100
num_expert_trajs: 5
  # - 10
  # - 25
airl_grad_steps: 5
airl_batch_size: 128
gail_grad_steps: 5
gail_batch_size: 128
expert_batch_size: 500

# PPO
normalize_obs: False
hidden_size: 256
lr: 3.0e-4
clip_param: 0.1
eps: 1.0e-5
alpha: 0.99
gamma: 0.99
use_gae: True
gae_lambda: 0.95
entropy_coef: 0.001
value_loss_coef: 0.5
max_grad_norm: 0.5
cuda_deterministic: False
use_proper_time_limits: True
recurrent_policy: False
use_linear_lr_decay: True
tanh_squash: True
policy_logstd_init: 0

# Logs
log_interval: 10
save_interval: 25000
eval_interval: null
